{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /home/vladimir/.local/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: transformer_lens in /home/vladimir/.local/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12>=8.9.2.26 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (8.9.2.26)\n",
      "Requirement already satisfied: triton>=2.1.0 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (4.8.0)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (2.15.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12>=12.1.105 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (11.4.5.107)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (0.2.24)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12>=12.1.105 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (13.7.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12>=10.3.2.106 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (10.3.2.106)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (4.35.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (0.13.10)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (0.25.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (4.66.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (2.1.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (11.0.2.54)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (1.26.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /home/vladimir/.local/lib/python3.10/site-packages (from transformer_lens) (2.18.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/vladimir/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/vladimir/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.1)\n",
      "Requirement already satisfied: psutil in /home/vladimir/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (5.9.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vladimir/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n",
      "Requirement already satisfied: xxhash in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.7)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.9.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/vladimir/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2023.10.0)\n",
      "Requirement already satisfied: typeguard<3,>=2.13.3 in /home/vladimir/.local/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vladimir/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12>=11.4.5.107->transformer_lens) (12.3.101)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/vladimir/.local/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vladimir/.local/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vladimir/.local/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vladimir/.local/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: sympy in /home/vladimir/.local/lib/python3.10/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/vladimir/.local/lib/python3.10/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.2)\n",
      "Requirement already satisfied: networkx in /home/vladimir/.local/lib/python3.10/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.2.1)\n",
      "Requirement already satisfied: filelock in /home/vladimir/.local/lib/python3.10/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/vladimir/.local/lib/python3.10/site-packages (from transformers>=4.25.1->transformer_lens) (0.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vladimir/.local/lib/python3.10/site-packages (from transformers>=4.25.1->transformer_lens) (2023.10.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (4.23.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.40)\n",
      "Requirement already satisfied: pathtools in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (59.6.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.38.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (8.0.3)\n",
      "Requirement already satisfied: setproctitle in /home/vladimir/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vladimir/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vladimir/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vladimir/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vladimir/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vladimir/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/vladimir/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/vladimir/.local/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vladimir/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vladimir/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vladimir/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vladimir/.local/lib/python3.10/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vladimir/.local/lib/python3.10/site-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/vladimir/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, tensor, arange, randn, randint, tril, where, full_like, ones, allclose, empty, zeros\n",
    "from torch.nn import Module, Linear, GELU, ReLU, Parameter, Embedding, ModuleList, MSELoss\n",
    "from torch.nn.functional import softmax, cross_entropy\n",
    "from torch.optim import AdamW \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformer_lens import HookedTransformer\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Callable\n",
    "from einops import einsum\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isfile\n",
    "from math import sqrt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int\n",
    "    ncontext: int\n",
    "    dmodel: int\n",
    "    dhead: int\n",
    "    nhead: int\n",
    "    dmlp : int\n",
    "    nlayers: int\n",
    "    activation_function: Callable = GELU()\n",
    "\n",
    "def normalize(x, dim=-1, eps=1e-5):\n",
    "    return x / (x.pow(2).mean(dim=dim, keepdim=True) + eps).sqrt()\n",
    "\n",
    "@dataclass\n",
    "class BlockActivations:\n",
    "    mid:       Optional[Tensor] = None\n",
    "    post:      Optional[Tensor] = None\n",
    "    attention: Optional[Tensor] = None\n",
    "    mlp:       Optional[Tensor] = None\n",
    "\n",
    "class BlockAutoencoders:\n",
    "    mid:       Callable = lambda self, x: x\n",
    "    post:      Callable = lambda self, x: x\n",
    "    attention: Callable = lambda self, x: x\n",
    "    mlp:       Callable = lambda self, x: x\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.up = Linear(cfg.dmodel, cfg.dmlp)\n",
    "        self.down = Linear(cfg.dmlp, cfg.dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.cfg.activation_function(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.query_weight  = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.key_weight    = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.value_weight  = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.output_weight = Parameter(randn(cfg.nhead, cfg.dhead, cfg.dmodel) / sqrt(cfg.nhead * cfg.dhead))\n",
    "\n",
    "        self.query_bias    = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.key_bias      = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.value_bias    = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.output_bias   = Parameter(randn(cfg.dmodel)           / sqrt(cfg.nhead * cfg.dhead))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ncontext = x.size(-2)\n",
    "\n",
    "        query = einsum(x, self.query_weight, \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        key   = einsum(x, self.key_weight,   \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        value = einsum(x, self.value_weight, \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        query = query + self.query_bias\n",
    "        key   = key   + self.key_bias\n",
    "        value = value + self.value_bias\n",
    "\n",
    "        attention = einsum(\n",
    "            key,\n",
    "            query,\n",
    "            \"... ncontext_key nhead dhead, ... ncontext_query nhead dhead -> ... nhead ncontext_query ncontext_key\"\n",
    "        )\n",
    "        attention = attention / sqrt(self.cfg.dhead)\n",
    "        attention_mask = tril(ones((ncontext, ncontext), dtype=torch.bool, device=device))\n",
    "        attention = where(attention_mask, attention, full_like(attention, -1e5))\n",
    "        attention = softmax(attention, dim=-1)\n",
    "        \n",
    "        output = einsum(\n",
    "            attention,\n",
    "            value,\n",
    "            \"... nhead ncontext_query ncontext_key, ... ncontext_key nhead dhead -> ... ncontext_query nhead dhead\"\n",
    "        )\n",
    "        result = einsum(output, self.output_weight, \"... ncontext nhead dhead, nhead dhead dmodel -> ... ncontext dmodel\")\n",
    "        result = result + self.output_bias\n",
    "        return result\n",
    "    \n",
    "class TransformerBlock(Module):\n",
    "    def __init__(self, cfg, autoencoders=BlockAutoencoders()):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.autoencoders = autoencoders\n",
    "\n",
    "        self.attention = Attention(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, pre, return_activations=False):\n",
    "        attention = self.attention(normalize(pre))\n",
    "        attention = self.autoencoders.attention(attention)\n",
    "        mid = pre + attention\n",
    "        mid = self.autoencoders.mid(mid)\n",
    "        mlp = self.mlp(normalize(mid))\n",
    "        mlp = self.autoencoders.mlp(mlp)\n",
    "        post = mid + mlp\n",
    "        post = self.autoencoders.post(post)\n",
    "\n",
    "        if return_activations:\n",
    "            return post, BlockActivations(mid=mid, post=post, attention=attention, mlp=mlp)\n",
    "        else:\n",
    "            return post\n",
    "\n",
    "class Transformer(Module):\n",
    "    def __init__(self, cfg, autoencoders=None, tokenizer=None):\n",
    "        super().__init__()\n",
    "        if type(cfg.activation_function) == str:\n",
    "            cfg.activation_function = {\"gelu\": GELU(), \"relu\": ReLU()}[cfg.activation_function]\n",
    "        if autoencoders is None:\n",
    "            autoencoders = [BlockAutoencoders() for _ in range(cfg.nlayers)]\n",
    "        assert len(autoencoders) == cfg.nlayers\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.embedding = Embedding(cfg.vocab_size, cfg.dmodel)\n",
    "        self.positional_embedding = Embedding(cfg.ncontext, cfg.dmodel)\n",
    "        self.blocks = ModuleList([TransformerBlock(cfg, autoencoders[i]) for i in range(cfg.nlayers)])\n",
    "        self.unembedding = Linear(cfg.dmodel, cfg.vocab_size)\n",
    "\n",
    "    def forward(self, x, return_activations=False, stop_at_layer=None):\n",
    "        if isinstance(x, str):\n",
    "            x = self.tokenizer(x)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        ncontext = x.size(-2)\n",
    "        x = x + self.positional_embedding(arange(ncontext, device=device))\n",
    "        \n",
    "        if not return_activations:\n",
    "            for layer, block in enumerate(self.blocks):\n",
    "                x = block(x)\n",
    "            \n",
    "                if stop_at_layer is not None and layer > stop_at_layer:\n",
    "                    break\n",
    "        else:\n",
    "            activations = []\n",
    "            for layer, block in enumerate(self.blocks):\n",
    "                x, block_activations = block(x, return_activations=True)\n",
    "                activations.append(block_activations)\n",
    "            \n",
    "                if stop_at_layer is not None and layer > stop_at_layer:\n",
    "                    break\n",
    "\n",
    "        x = normalize(x)\n",
    "        x = self.unembedding(x)\n",
    "        \n",
    "        if not return_activations:\n",
    "            return x\n",
    "        else:\n",
    "            return x, activations\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name):\n",
    "        theirs = HookedTransformer.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        ours = Transformer(TransformerConfig( vocab_size=theirs.tokenizer.vocab_size,\n",
    "                                      ncontext=theirs.cfg.n_ctx,\n",
    "                                      dmodel=theirs.cfg.d_model,\n",
    "                                      dhead=theirs.cfg.d_head,\n",
    "                                      nhead=theirs.cfg.n_heads,\n",
    "                                      dmlp=theirs.cfg.d_mlp,\n",
    "                                      nlayers=theirs.cfg.n_layers,\n",
    "                                      activation_function=theirs.cfg.act_fn ))\n",
    "\n",
    "        ours.tokenizer = theirs.tokenizer\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ours.embedding.weight.copy_(theirs.embed.W_E)\n",
    "            ours.positional_embedding.weight.copy_(theirs.pos_embed.W_pos)\n",
    "            ours.unembedding.weight.copy_(theirs.unembed.W_U.transpose(0, 1))\n",
    "            ours.unembedding.bias.copy_(theirs.unembed.b_U)\n",
    "            \n",
    "            for layer in range(ours.cfg.nlayers):\n",
    "                ours.blocks[layer].attention.query_weight.copy_(theirs.blocks[layer].attn.W_Q)\n",
    "                ours.blocks[layer].attention.key_weight.copy_(theirs.blocks[layer].attn.W_K)\n",
    "                ours.blocks[layer].attention.value_weight.copy_(theirs.blocks[layer].attn.W_V)\n",
    "                ours.blocks[layer].attention.output_weight.copy_(theirs.blocks[layer].attn.W_O)\n",
    "\n",
    "                ours.blocks[layer].attention.query_bias.copy_(theirs.blocks[layer].attn.b_Q)\n",
    "                ours.blocks[layer].attention.key_bias.copy_(theirs.blocks[layer].attn.b_K)\n",
    "                ours.blocks[layer].attention.value_bias.copy_(theirs.blocks[layer].attn.b_V)\n",
    "                ours.blocks[layer].attention.output_bias.copy_(theirs.blocks[layer].attn.b_O)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.weight.copy_(theirs.blocks[layer].mlp.W_in.transpose(0, 1))\n",
    "                ours.blocks[layer].mlp.down.weight.copy_(theirs.blocks[layer].mlp.W_out.transpose(0, 1))\n",
    "\n",
    "                ours.blocks[layer].mlp.up.bias.copy_(theirs.blocks[layer].mlp.b_in)\n",
    "                ours.blocks[layer].mlp.down.bias.copy_(theirs.blocks[layer].mlp.b_out)\n",
    "\n",
    "        return ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Loaded pretrained model gelu-1l into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = Transformer.from_pretrained(\"gelu-1l\")\n",
    "model.eval()\n",
    "\n",
    "their_model = HookedTransformer.from_pretrained(\"gelu-1l\")\n",
    "their_model.eval()\n",
    "input = randint(their_model.tokenizer.vocab_size, (64, 64))\n",
    "assert(allclose(their_model(input), model(input), atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset, val_size=0.1, test_size=0.1):\n",
    "    assert set(dataset.keys()) == {\"train\"}\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=val_size+test_size)\n",
    "    val_test_dataset = dataset[\"test\"].train_test_split(test_size = val_size / (val_size + test_size))\n",
    "    return DatasetDict({ \"train\": dataset[\"train\"],\n",
    "                         \"val\":   val_test_dataset[\"train\"],\n",
    "                         \"test\":  val_test_dataset[\"test\"] })\n",
    "\n",
    "def make_tokens_dataset(text_dataset, tokenizer, ncontext, _tqdm=True, max_size=None, save_to=None):\n",
    "    if save_to is not None and isfile(save_to):\n",
    "        print(f\"Loading tokens dataset from file '{save_to}'.\")\n",
    "        return torch.load(save_to)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        print(\"WARNING: tqdm doesn't work properly when max_size is not None\") # we don't care because max_size is only temporal\n",
    "\n",
    "    token_seqs = []\n",
    "    for x in tqdm(text_dataset) if _tqdm else text_dataset:\n",
    "        tokens = tokenizer(x[\"text\"])[\"input_ids\"]\n",
    "        if len(tokens) <= ncontext:\n",
    "            continue\n",
    "        tokens = tokens[:ncontext]\n",
    "        token_seqs.append(tokens)\n",
    "        \n",
    "        if max_size is not None and len(token_seqs) >= max_size:\n",
    "            break\n",
    "    \n",
    "    dataset = TensorDataset(tensor(token_seqs))\n",
    "\n",
    "    if save_to is not None:\n",
    "        torch.save(dataset, save_to)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class BlockActivationsDataset(Dataset):\n",
    "    def __init__(self, activations: BlockActivations):\n",
    "        self.activations = activations\n",
    "\n",
    "    def __len__(self):\n",
    "        for attr in [\"mid\", \"post\", \"attention\", \"mlp\"]:\n",
    "            activation = getattr(self.activations, attr)\n",
    "            if activation is not None:\n",
    "                return activation.size(0)\n",
    "        assert False\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice = BlockActivations(None, None, None, None)\n",
    "        for attr in [\"mid\", \"post\", \"attention\", \"mlp\"]:\n",
    "            activation = getattr(self.activations, attr)\n",
    "            if activation is not None:\n",
    "                setattr(slice, attr, activation[index])\n",
    "        return slice\n",
    "\n",
    "class ActivationsDataset(Dataset):\n",
    "    def __init__(self, activations: Dict[int, BlockActivations]):\n",
    "        self.activations = { layer: BlockActivationsDataset(layer_activations)\n",
    "                             for layer, layer_activations in activations.items() }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.activations.values())))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {layer: activations[index] for layer, activations in self.activations.items()}\n",
    "\n",
    "def block_activations_collate_fn(activations: List[BlockActivations]):\n",
    "    checkpoints = [checkpoint for checkpoint in [\"mid\", \"post\", \"attention\", \"mlp\"] if getattr(activations[0], checkpoint) is not None]\n",
    "    shape = (len(activations), *getattr(activations[0], checkpoints[0]).shape)\n",
    "    result = BlockActivations(**{checkpoint: empty(shape) for checkpoint in checkpoints})\n",
    "    for i, acts in enumerate(activations):\n",
    "        for checkpoint in checkpoints:\n",
    "            getattr(result, checkpoint)[i] = getattr(acts, checkpoint)\n",
    "    return result\n",
    "\n",
    "def activations_collate_fn(activations: List[Dict[int, BlockActivations]]):\n",
    "    return {layer: block_activations_collate_fn([acts[layer] for acts in activations]) for layer in activations[0].keys()}\n",
    "\n",
    "def make_activation_dataset(model, dataloader, layers, checkpoints, _tqdm=True, save_to=None):\n",
    "    if save_to is not None and isfile(save_to):\n",
    "        print(f\"Loading activations dataset from file '{save_to}'.\")\n",
    "        return torch.load(save_to)\n",
    "\n",
    "    all_activations = None\n",
    "    i = 0\n",
    "    for data, in tqdm(dataloader) if _tqdm else dataloader:\n",
    "        _, activations = model(data, return_activations=True, stop_at_layer=max(layers))\n",
    "        \n",
    "        if all_activations is None:\n",
    "            shape = (len(dataloader.dataset), *activations[0].mid.shape[1:])\n",
    "            all_activations = { layer: BlockActivations(**{checkpoint: empty(shape) for checkpoint in checkpoints})\n",
    "                                for layer in layers }\n",
    "            \n",
    "        batch_size = activations[0].mid.size(0)\n",
    "        for layer in layers:\n",
    "            for checkpoint in checkpoints:\n",
    "                getattr(all_activations[layer], checkpoint)[i:i+batch_size] = getattr(activations[layer], checkpoint)\n",
    "        \n",
    "        i += batch_size\n",
    "\n",
    "    dataset = ActivationsDataset(all_activations)\n",
    "    assert i == len(dataset)\n",
    "\n",
    "    if save_to is not None:\n",
    "        torch.save(dataset, save_to)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"maxtli/OpenWebText-2M\")\n",
    "dataset = train_val_test_split(dataset)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset   = dataset[\"val\"]\n",
    "test_dataset  = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tqdm doesn't work properly when max_size is not None\n",
      "Loading tokens dataset from file tokens_dataset.pickle.\n",
      "Loading activations dataset from file activations_dataset.pickle.\n"
     ]
    }
   ],
   "source": [
    "tokens_dataset = make_tokens_dataset(train_dataset, model.tokenizer, ncontext=250, max_size=1_000, save_to=\"tokens_dataset.pickle\")\n",
    "tokens_dataloader = DataLoader(tokens_dataset, batch_size=64, shuffle=True)\n",
    "activations_dataset = make_activation_dataset(model, tokens_dataloader, layers=[0], checkpoints=[\"mlp\"], save_to=\"activations_dataset.pickle\")\n",
    "activations_dataloader = DataLoader(activations_dataset, batch_size=64, collate_fn=activations_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_logits(model, seq):\n",
    "    return model(seq)[..., -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_dataset(vocab_size, ncontext, size):\n",
    "    assert ncontext % 2 == 1\n",
    "    data = randint(vocab_size, (size, (ncontext + 1) // 2), device=device)\n",
    "    data = data.repeat(1, 2)\n",
    "    return TensorDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cross_entropy_loss(pred, true):\n",
    "    return cross_entropy(pred.transpose(1, -1), true.transpose(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, loss_fn=transformer_cross_entropy_loss, lr=1e-3, epoch_tqdm=True, batch_tqdm=False, plot_loss=True):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in tqdm(range(epochs)) if epoch_tqdm else range(epochs):\n",
    "        for x, in tqdm(dataloader) if batch_tqdm else dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x[..., :-1]), x[..., 1:])\n",
    "            loss_history.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.title(\"training_loss\")\n",
    "        plt.xlabel(\"training iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/7813 [00:00<02:24, 54.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 552/7813 [00:05<01:18, 92.57it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(repetition_dataset(vocab_size\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mvocab_size, ncontext\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mncontext, size\u001b[39m=\u001b[39m\u001b[39m500_000\u001b[39m), batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Transformer(cfg)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(model, train_dataloader, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_tqdm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epoch_tqdm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, \u001b[39min\u001b[39;00m tqdm(dataloader) \u001b[39mif\u001b[39;00m batch_tqdm \u001b[39melse\u001b[39;00m dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(model(x[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_history\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_activations:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m layer \u001b[39m>\u001b[39m stop_at_layer:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pre, return_activations\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(normalize(pre))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautoencoders\u001b[39m.\u001b[39mattention(attention)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     mid \u001b[39m=\u001b[39m pre \u001b[39m+\u001b[39m attention\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     ncontext \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     query \u001b[39m=\u001b[39m einsum(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_weight, \u001b[39m\"\u001b[39;49m\u001b[39m... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     key   \u001b[39m=\u001b[39m einsum(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_weight,   \u001b[39m\"\u001b[39m\u001b[39m... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     value \u001b[39m=\u001b[39m einsum(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_weight, \u001b[39m\"\u001b[39m\u001b[39m... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py:901\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    899\u001b[0m tensors \u001b[39m=\u001b[39m tensors_and_pattern[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    900\u001b[0m pattern \u001b[39m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n\u001b[0;32m--> 901\u001b[0m \u001b[39mreturn\u001b[39;00m get_backend(tensors[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49meinsum(pattern, \u001b[39m*\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/einops/_backends.py:287\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, pattern, *x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, pattern, \u001b[39m*\u001b[39mx):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(pattern, \u001b[39m*\u001b[39;49mx)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = TransformerConfig(vocab_size=10, ncontext=17, dmodel=16, dhead=4, nhead=4, dmlp=32, nlayers=2)\n",
    "train_dataloader = DataLoader(repetition_dataset(vocab_size=cfg.vocab_size, ncontext=cfg.ncontext, size=500_000), batch_size=64, shuffle=True)\n",
    "model = Transformer(cfg).to(device)\n",
    "train(model, train_dataloader, epochs=1, batch_tqdm=True, epoch_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 18])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 2, 5, 2, 1, 3, 1, 6])\n",
      "tensor([8, 2, 5, 2, 1, 3, 1, 6, 1, 8, 2, 5, 2, 1, 3, 1, 6])\n",
      "tensor([1, 8, 2, 5, 2, 1, 3, 1, 6, 1, 8, 2, 5, 2, 1, 3, 1])\n",
      "tensor(1.0842, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(repetition_dataset(vocab_size=cfg.vocab_size, ncontext=cfg.ncontext, size=1_000), batch_size=64, shuffle=True)\n",
    "x, = next(iter(test_dataloader))\n",
    "model(x[0, :2])\n",
    "print(x.shape)\n",
    "print(model(x[..., :-1]).argmax(-1)[0, ...])\n",
    "print(x[..., 1:][0, ...])\n",
    "print(x[..., :-1][0, ...])\n",
    "print(transformer_cross_entropy_loss(model(x[..., :-1]), x[..., 1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
