{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.35.2\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: more_itertools in /usr/lib/python3/dist-packages (4.2.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (2022.10.31)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (2.28.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.35.2)\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.9/site-packages (from transformers==4.35.2) (4.64.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2)\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.9/site-packages (from requests->transformers==4.35.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.35.2) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.9/site-packages (from requests->transformers==4.35.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.35.2) (2019.11.28)\n",
      "Installing collected packages: safetensors, fsspec, einops, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.11.1\n",
      "    Uninstalling huggingface-hub-0.11.1:\n",
      "      Successfully uninstalled huggingface-hub-0.11.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "Successfully installed einops-0.7.0 fsspec-2023.12.1 huggingface-hub-0.19.4 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install einops transformers==4.35.2 more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, tensor, arange, randn, randint, tril, where, ones, allclose, empty, zeros, inference_mode, Storage, FloatTensor\n",
    "from torch.nn import Module, Linear, GELU, ReLU, Parameter, Embedding, ModuleList, LayerNorm, MSELoss, KLDivLoss\n",
    "from torch.nn.functional import softmax, cross_entropy\n",
    "from torch.nn.init import zeros_\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "# from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from copy import copy\n",
    "from typing import Optional, Tuple, List, Dict, Callable, Iterable, Any\n",
    "from einops import einsum\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isfile\n",
    "from math import sqrt, pi, prod\n",
    "from more_itertools import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int\n",
    "    ncontext: int\n",
    "    dmodel: int\n",
    "    dhead: int\n",
    "    nhead: int\n",
    "    dmlp : int\n",
    "    nlayers: int\n",
    "    activation_function: Callable = GELU()\n",
    "    mask_value: float = 1e-5\n",
    "    attention_scale: float = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.attention_scale is None:\n",
    "            self.attention_scale = 1 / sqrt(self.dhead)\n",
    "\n",
    "# def normalize(x, dim=-1, eps=1e-5):\n",
    "#     return x / (x.pow(2).mean(dim=dim, keepdim=True) + eps).sqrt()\n",
    "\n",
    "# copy pasted from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "class NewGELUActivation(Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return 0.5 * input * (1.0 + torch.tanh(sqrt(2.0 / pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.up = Linear(cfg.dmodel, cfg.dmlp)\n",
    "        self.down = Linear(cfg.dmlp, cfg.dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.cfg.activation_function(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.query_weight  = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.key_weight    = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.value_weight  = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.output_weight = Parameter(randn(cfg.nhead, cfg.dhead, cfg.dmodel) / sqrt(cfg.nhead * cfg.dhead))\n",
    "\n",
    "        self.query_bias    = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.key_bias      = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.value_bias    = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.output_bias   = Parameter(randn(cfg.dmodel)           / sqrt(cfg.nhead * cfg.dhead))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ncontext = x.size(-2)\n",
    "\n",
    "        query = einsum(x, self.query_weight, \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        key   = einsum(x, self.key_weight,   \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        value = einsum(x, self.value_weight, \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        query = query + self.query_bias\n",
    "        key   = key   + self.key_bias\n",
    "        value = value + self.value_bias\n",
    "\n",
    "        attention = einsum(\n",
    "            key,\n",
    "            query,\n",
    "            \"... ncontext_key nhead dhead, ... ncontext_query nhead dhead -> ... nhead ncontext_query ncontext_key\"\n",
    "        )\n",
    "        attention = self.cfg.attention_scale * attention\n",
    "        attention_mask = tril(ones((ncontext, ncontext), dtype=torch.bool, device=device))\n",
    "        attention = where(attention_mask, attention, tensor(self.cfg.mask_value, device=device))\n",
    "        attention = softmax(attention, dim=-1)\n",
    "        \n",
    "        output = einsum(\n",
    "            attention,\n",
    "            value,\n",
    "            \"... nhead ncontext_query ncontext_key, ... ncontext_key nhead dhead -> ... ncontext_query nhead dhead\"\n",
    "        )\n",
    "        result = einsum(output, self.output_weight, \"... ncontext nhead dhead, nhead dhead dmodel -> ... ncontext dmodel\")\n",
    "        result = result + self.output_bias\n",
    "        return result\n",
    "    \n",
    "@dataclass\n",
    "class BlockOutput:\n",
    "    output:                  Tensor\n",
    "    activations:             Optional[Dict[str, Tensor]] = None\n",
    "    autoencoder_activations: Optional[Dict[str, Tensor]] = None\n",
    "\n",
    "class TransformerBlock(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.attention_layer_norm = LayerNorm(cfg.dmodel)\n",
    "        self.mlp_layer_norm = LayerNorm(cfg.dmodel)\n",
    "\n",
    "        self.attention = Attention(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, pre, return_activations=False, return_autoencoder_activations=False, autoencoders=dict()):\n",
    "        assert set(autoencoders.keys()) <= {\"pre\", \"mid\", \"post\", \"attentio\", \"mlp\"}\n",
    "\n",
    "        autoencoder_outputs = dict()\n",
    "\n",
    "        if \"pre\" in autoencoders:\n",
    "            pre, autoencoder_outputs[\"pre\"] = autoencoders[\"pre\"](pre)\n",
    "\n",
    "        attention = self.attention(self.attention_layer_norm(pre))\n",
    "        \n",
    "        if \"attention\" in autoencoders:\n",
    "            attention, autoencoder_outputs[\"attention\"] = autoencoders[\"attention\"](attention)\n",
    "        \n",
    "        mid = pre + attention\n",
    "        \n",
    "        if \"mid\" in autoencoders:\n",
    "            mid, autoencoder_outputs[\"mid\"] = autoencoders[\"mid\"]\n",
    "        \n",
    "        mlp = self.mlp(self.mlp_layer_norm(mid))\n",
    "        \n",
    "        if \"mlp\" in autoencoders:\n",
    "            mid, autoencoder_outputs[\"mlp\"] = autoencoders[\"mlp\"]\n",
    "        \n",
    "        post = mid + mlp\n",
    "        \n",
    "        if \"post\" in autoencoders:\n",
    "            post, autoencoder_outputs[\"post\"] = autoencoders[\"post\"]\n",
    "\n",
    "        activations = {\"mid\": mid, \"post\": post, \"attention\": attention, mlp: mlp}\n",
    "\n",
    "        return BlockOutput(\n",
    "            output=                  post,\n",
    "            activations=             activations if return_activations else None,\n",
    "            autoencoder_activations= autoencoder_outputs if return_autoencoder_activations else None\n",
    "        )\n",
    "    \n",
    "@dataclass\n",
    "class TransformerOutput:\n",
    "    logits:                  Tensor\n",
    "    activations:             Optional[Dict[Tuple[int, str], Tensor]] = None\n",
    "    autoencoder_activations: Optional[Dict[Tuple[int, str], Tensor]] = None\n",
    "\n",
    "class Transformer(Module):\n",
    "    def __init__(self, cfg, tokenizer=None):\n",
    "        super().__init__()\n",
    "        if type(cfg.activation_function) == str:\n",
    "            cfg.activation_function = {\"gelu\": GELU(), \"relu\": ReLU(), \"gelu_new\": NewGELUActivation()}[cfg.activation_function]\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.embedding = Embedding(cfg.vocab_size, cfg.dmodel)\n",
    "        self.positional_embedding = Embedding(cfg.ncontext, cfg.dmodel)\n",
    "        self.blocks = ModuleList([TransformerBlock(cfg) for _ in range(cfg.nlayers)])\n",
    "        self.unembedding = Linear(cfg.dmodel, cfg.vocab_size)\n",
    "        self.final_layer_norm = LayerNorm(cfg.dmodel)\n",
    "\n",
    "    def forward( self,\n",
    "                 x,\n",
    "                 return_activations=False,\n",
    "                 return_autoencoder_activations=False,\n",
    "                 stop_at_layer=None,\n",
    "                 autoencoders: Dict[Tuple[int, str], Callable] = dict() ):\n",
    "\n",
    "        assert all( layer in range(self.cfg.nlayers) and checkpoint in [\"pre\", \"mid\", \"post\", \"mlp\", \"attention\"]\n",
    "                    for layer, checkpoint in autoencoders.keys() )\n",
    "\n",
    "        if isinstance(x, str):\n",
    "            x = self.tokenizer(x)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        ncontext = x.size(-2)\n",
    "        x = x + self.positional_embedding(arange(ncontext, device=device))\n",
    "        \n",
    "        activations = dict() if return_activations else None\n",
    "        autoencoder_activations = dict() if return_autoencoder_activations else None\n",
    "        \n",
    "        blocks = self.blocks if stop_at_layer is None else self.blocks[:stop_at_layer]\n",
    "        for layer, block in enumerate(blocks):\n",
    "            pre = x\n",
    "\n",
    "            if layer == 0 and (0, \"pre\") in autoencoders:\n",
    "                pre, autoencoder_activations[(0, \"pre\")] = autoencoders[(0, \"pre\")](pre)\n",
    "\n",
    "            auoencoders_on_layer = { checkpoint: autoencoder\n",
    "                                     for (layer_, checkpoint), autoencoder in autoencoders\n",
    "                                     if layer_ == layer }\n",
    "            output = block( x,\n",
    "                            return_activations=             return_activations,\n",
    "                            return_autoencoder_activations= autoencoder_activations,\n",
    "                            autoencoders=                   autoencoders_on_layer)\n",
    "\n",
    "            if return_activations:\n",
    "                for checkpoint, activation in output.activations:\n",
    "                    activations[(layer, checkpoint)] = activation\n",
    "                if layer == 0:\n",
    "                    activations[(0, \"pre\")] = pre\n",
    "\n",
    "            if return_autoencoder_activations:\n",
    "                for checpoint, activation in output.autoencoder_activations:\n",
    "                    activations[(layer, checkpoint)] = activation\n",
    "\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.unembedding(x)\n",
    "        \n",
    "        return TransformerOutput(logits=x, activations=activations, autoencoder_activations=autoencoder_activations)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name, test=True, test_atol=1e-4):\n",
    "        theirs = AutoModelForCausalLM.from_pretrained(pretrained_model_name) # .to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "        ours = Transformer(TransformerConfig( vocab_size=          tokenizer.vocab_size,\n",
    "                                              ncontext=            theirs.config.max_position_embeddings,\n",
    "                                              dmodel=              theirs.config.hidden_size,\n",
    "                                              dhead=               theirs.config.hidden_size // theirs.config.num_heads,\n",
    "                                              nhead=               theirs.config.num_heads,\n",
    "                                              dmlp=                4 * theirs.config.hidden_size,\n",
    "                                              nlayers=             theirs.config.num_layers,\n",
    "                                              activation_function= theirs.config.activation_function,\n",
    "                                              attention_scale=     1.0,\n",
    "                                              mask_value=          torch.finfo(torch.float).min )).to(device)\n",
    "        \n",
    "        ours.tokenizer = tokenizer\n",
    "\n",
    "        with inference_mode():\n",
    "            ours.embedding.weight.copy_(theirs.transformer.wte.weight)\n",
    "            ours.positional_embedding.weight.copy_(theirs.transformer.wpe.weight)\n",
    "            ours.unembedding.weight.copy_(theirs.transformer.wte.weight)\n",
    "            zeros_(ours.unembedding.bias)\n",
    "            ours.final_layer_norm.weight.copy_(theirs.transformer.ln_f.weight)\n",
    "            ours.final_layer_norm.bias.copy_(theirs.transformer.ln_f.bias)\n",
    "                    \n",
    "            for layer in range(ours.cfg.nlayers):\n",
    "                ours.blocks[layer].attention_layer_norm.weight.copy_(theirs.transformer.h[layer].ln_1.weight)\n",
    "                ours.blocks[layer].attention_layer_norm.bias.copy_(theirs.transformer.h[layer].ln_1.bias)\n",
    "                ours.blocks[layer].mlp_layer_norm.weight.copy_(theirs.transformer.h[layer].ln_2.weight)\n",
    "                ours.blocks[layer].mlp_layer_norm.bias.copy_(theirs.transformer.h[layer].ln_2.bias)\n",
    "\n",
    "                ours.blocks[layer].attention.query_weight.copy_(theirs.transformer.h[layer].attn.attention.q_proj.weight.reshape(ours.cfg.nhead, ours.cfg.dhead, ours.cfg.dmodel).permute(0, 2, 1))\n",
    "                ours.blocks[layer].attention.key_weight.copy_(theirs.transformer.h[layer].attn.attention.k_proj.weight.reshape(ours.cfg.nhead, ours.cfg.dhead, ours.cfg.dmodel).permute(0, 2, 1))\n",
    "                ours.blocks[layer].attention.value_weight.copy_(theirs.transformer.h[layer].attn.attention.v_proj.weight.reshape(ours.cfg.nhead, ours.cfg.dhead, ours.cfg.dmodel).permute(0, 2, 1))\n",
    "                ours.blocks[layer].attention.output_weight.copy_(theirs.transformer.h[layer].attn.attention.out_proj.weight.reshape(ours.cfg.dmodel, ours.cfg.nhead, ours.cfg.dhead).permute(1, 2, 0))\n",
    "\n",
    "                zeros_(ours.blocks[layer].attention.query_bias)\n",
    "                zeros_(ours.blocks[layer].attention.key_bias)\n",
    "                zeros_(ours.blocks[layer].attention.value_bias)\n",
    "                ours.blocks[layer].attention.output_bias.copy_(theirs.transformer.h[layer].attn.attention.out_proj.bias)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.weight.copy_(theirs.transformer.h[layer].mlp.c_fc.weight)\n",
    "                ours.blocks[layer].mlp.down.weight.copy_(theirs.transformer.h[layer].mlp.c_proj.weight)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.bias.copy_(theirs.transformer.h[layer].mlp.c_fc.bias)\n",
    "                ours.blocks[layer].mlp.down.bias.copy_(theirs.transformer.h[layer].mlp.c_proj.bias)\n",
    "            \n",
    "\n",
    "        if test:\n",
    "            with inference_mode():\n",
    "                print(\"Testing that the model behaves the same as the library model... \", end=\"\", flush=True)\n",
    "                inputs = randint(0, ours.cfg.vocab_size, (64, 64), device=device)\n",
    "                assert allclose(ours(inputs).logits, theirs(inputs).logits, atol=test_atol), \"Tests failed!\"\n",
    "                print(\"Test passed!\")\n",
    "\n",
    "        return ours\n",
    "    \n",
    "        \"\"\"\n",
    "        theirs = HookedTransformer.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        ours = Transformer(TransformerConfig( vocab_size=theirs.tokenizer.vocab_size,\n",
    "                                      ncontext=theirs.cfg.n_ctx,\n",
    "                                      dmodel=theirs.cfg.d_model,\n",
    "                                      dhead=theirs.cfg.d_head,\n",
    "                                      nhead=theirs.cfg.n_heads,\n",
    "                                      dmlp=theirs.cfg.d_mlp,\n",
    "                                      nlayers=theirs.cfg.n_layers,\n",
    "                                      activation_function=theirs.cfg.act_fn ))\n",
    "\n",
    "        ours.tokenizer = theirs.tokenizer\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ours.embedding.weight.copy_(theirs.embed.W_E)\n",
    "            ours.positional_embedding.weight.copy_(theirs.pos_embed.W_pos)\n",
    "            ours.unembedding.weight.copy_(theirs.unembed.W_U.transpose(0, 1))\n",
    "            ours.unembedding.bias.copy_(theirs.unembed.b_U)\n",
    "            \n",
    "            for layer in range(ours.cfg.nlayers):\n",
    "                ours.blocks[layer].attention.query_weight.copy_(theirs.blocks[layer].attn.W_Q)\n",
    "                ours.blocks[layer].attention.key_weight.copy_(theirs.blocks[layer].attn.W_K)\n",
    "                ours.blocks[layer].attention.value_weight.copy_(theirs.blocks[layer].attn.W_V)\n",
    "                ours.blocks[layer].attention.output_weight.copy_(theirs.blocks[layer].attn.W_O)\n",
    "\n",
    "                ours.blocks[layer].attention.query_bias.copy_(theirs.blocks[layer].attn.b_Q)\n",
    "                ours.blocks[layer].attention.key_bias.copy_(theirs.blocks[layer].attn.b_K)\n",
    "                ours.blocks[layer].attention.value_bias.copy_(theirs.blocks[layer].attn.b_V)\n",
    "                ours.blocks[layer].attention.output_bias.copy_(theirs.blocks[layer].attn.b_O)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.weight.copy_(theirs.blocks[layer].mlp.W_in.transpose(0, 1))\n",
    "                ours.blocks[layer].mlp.down.weight.copy_(theirs.blocks[layer].mlp.W_out.transpose(0, 1))\n",
    "\n",
    "                ours.blocks[layer].mlp.up.bias.copy_(theirs.blocks[layer].mlp.b_in)\n",
    "                ours.blocks[layer].mlp.down.bias.copy_(theirs.blocks[layer].mlp.b_out)\n",
    "\n",
    "        return ours\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtokenizer(\u001b[39m\"\u001b[39;49m\u001b[39mhello\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2907\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2908\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2909\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2910\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2911\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2912\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2913\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2914\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2915\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2916\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2917\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2918\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2919\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2920\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2921\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2922\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2968\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2947\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2948\u001b[0m \u001b[39mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   2949\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2964\u001b[0m \u001b[39m        method).\u001b[39;00m\n\u001b[1;32m   2965\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2967\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[1;32m   2969\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2970\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2971\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2972\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2973\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2974\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2975\u001b[0m )\n\u001b[1;32m   2977\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[1;32m   2978\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2979\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2995\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2996\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2703\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m-> 2703\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2705\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2706\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2707\u001b[0m     )\n\u001b[1;32m   2709\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2711\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2712\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2716\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd12011c41747f1ad8dfdddc001e8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea64df365b094582aa7a580927cdc603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/48.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bbb746c5194151ba0b49bd6009e387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaa5b10a66c44908dfbe7ba37fd10f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ee1e8f8b76418a83da2ee1cd7523d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2972c16e7c254c2497355297d0c2777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906b3f20d7674b1ca2ed097a017df105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(50257, 64)\n",
       "  (positional_embedding): Embedding(2048, 64)\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (down): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (unembedding): Linear(in_features=64, out_features=50257, bias=True)\n",
       "  (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer.from_pretrained(\"roneneldan/TinyStories-1M\", test=False).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val_test_split(dataset, val_size=0.1, test_size=0.1):\n",
    "#     dataset = dataset.train_test_split(test_size=val_size+test_size)\n",
    "#     val_test_dataset = dataset[\"test\"].train_test_split(test_size = val_size / (val_size + test_size))\n",
    "#     return DatasetDict({ \"train\": dataset[\"train\"],\n",
    "#                          \"val\":   val_test_dataset[\"train\"],\n",
    "#                          \"test\":  val_test_dataset[\"test\"] })\n",
    "\n",
    "def make_tokens_dataset(text_dataset, tokenizer, ncontext, _tqdm=True, max_size=None, save_to=None):\n",
    "    if save_to is not None and isfile(save_to):\n",
    "        print(f\"Loading tokens dataset from file '{save_to}'.\")\n",
    "        return torch.load(save_to)\n",
    "    \n",
    "    if _tqdm and max_size is not None:\n",
    "        print(\"WARNING: tqdm doesn't work properly when max_size is not None\")\n",
    "\n",
    "    token_seqs = []\n",
    "    for x in tqdm(text_dataset) if _tqdm else text_dataset:\n",
    "        tokens = tokenizer(x[\"text\"])[\"input_ids\"]\n",
    "        if len(tokens) <= ncontext:\n",
    "            continue\n",
    "        tokens = tokens[:ncontext]\n",
    "        token_seqs.append(tokens)\n",
    "        \n",
    "        if max_size is not None and len(token_seqs) >= max_size:\n",
    "            break\n",
    "    \n",
    "    dataset = TensorDataset(tensor(token_seqs, requires_grad=False))\n",
    "\n",
    "    if save_to is not None:\n",
    "        torch.save(dataset, save_to)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def all_equal(xs):\n",
    "    return all(current == next for current, next in zipnext(xs))\n",
    "\n",
    "class DictTensorDataset(Dataset):\n",
    "    def __init__(self, tensors: Dict[Any, Tensor]):\n",
    "        assert all_equal(tensor.size(0) for tensor in tensors.values()), \"Size mismatch between tensors\"\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return next(iter(self.tensors.values())).size(0)\n",
    "\n",
    "    def __getitem(self, index):\n",
    "        return {key: tensor[index] for key, tensor in self.tensors.items()}\n",
    "\n",
    "def dict_collate_fn(dicts: Iterable[Dict[Any, Tensor]]):\n",
    "    if not isinstance(dicts, list):\n",
    "        dicts = list(dicts)\n",
    "\n",
    "    collated = { key: empty(len(dicts), *tensor.shape)\n",
    "                 for key, tensor in next(iter(dicts)) }\n",
    "\n",
    "    for i, dict in enumerate(dicts):\n",
    "        for key, tensor in dicts:\n",
    "            collated[key][i, :] = tensor\n",
    "\n",
    "    return collated\n",
    "\n",
    "def list_to_english(xs, comma=\", \", and_=\" and \", oxford_and=\", and \"):\n",
    "    if len(xs) == 0:\n",
    "        return \"\"\n",
    "    if len(xs) == 1:\n",
    "        return xs[0]\n",
    "    if len(xs) == 2:\n",
    "        return xs[0] + and_ + xs[1]\n",
    "    return comma.join(xs[:-2]) + oxford_and + xs[-1]\n",
    "\n",
    "def make_activation_dataset(model, tokens_dataloader, checkpoints, on_disk=False, storage_directory=None, _tqdm=True):\n",
    "    with inference_mode():\n",
    "        dataset_size = tokens_dataloader.dataset.shape(0)\n",
    "        _, ncontext, dmodel = model(next(iter(tokens_dataloader)), return_activations=True).activations[next(iter(checkpoints))]\n",
    "\n",
    "        def storage_filename(checkpoint: Tuple[int, str]):\n",
    "            assert storage_directory is not None\n",
    "            return f\"{storage_directory}/layer{checkpoint[0]}-{checkpoint[1]}.dat\"\n",
    "        already_computed_checkpoints_list_filename = f\"{storage_directory}/already_computed_checkpoints_list.pickle\"\n",
    "\n",
    "        shape = (dataset_size, ncontext, dmodel)\n",
    "        if not on_disk:\n",
    "            dataset_activations = {checkpoint: empty(shape) for checkpoint in checkpoints}\n",
    "        else:\n",
    "            # same as the then branch, but stored on disk\n",
    "\n",
    "            if isfile(already_computed_checkpoints_list_filename):\n",
    "                with open(already_computed_checkpoints_list_filename, \"wb\") as file:\n",
    "                    already_computed_checkpoints = pickle.load(file)\n",
    "            else:\n",
    "                already_computed_checkpoints = []\n",
    "            \n",
    "            print(\"Loading activations\", list_to_english(f\"'{checkpoint}'\" for checkpoint in checkpoints_already_on_disk), \"from file.\")\n",
    "            \n",
    "            checkpoints = [checkpoint for checkpoint in checkpoints if checkpoint not in checkpoints_already_on_disk]\n",
    "            \n",
    "            dataset_activations = { checkpoint: FloatTensor(Storage.from_file(storage_filename(checkpoint), True, prod.shape)).reshape(size)\n",
    "                                    for checkpoint in checkpoints + already_computed_checkpoints }\n",
    "\n",
    "            for checkpoint in copy(checkpoints_already_on_disk):\n",
    "                if (dataset_activations[checkpoint][-1, :] == 0).all():\n",
    "                    print( \"The activations file for '{checkpoint}' seems weird - the last datapoint is all zeros.\" +\n",
    "                           \"The program was probably interrupted before it finished being computed.\" +\n",
    "                           \"It will be recalculated from scratch\" )\n",
    "                    checkpoints_already_on_disk.remove(checkpoint)\n",
    "                    checkpoints.append(checkpoints)\n",
    "\n",
    "        if len(checkpoints) > 0:\n",
    "            i = 0\n",
    "            for tokens in tokens_dataloader:\n",
    "                output = model(tokens, return_activations=True, stop_at_layer=1+max(layer for layer, _ in checkpoints))\n",
    "                batch_size = output.activations[(0, \"pre\")].size(0)\n",
    "                for checkpoint in checkpoints:\n",
    "                    dataset_activations[checkpoint][i:i+batch_size] = output.activations[checkpoint]\n",
    "                i += batch_size\n",
    "\n",
    "        if on_disk:\n",
    "            with open(already_computed_checkpoints_list_filename, \"wb\") as file:\n",
    "                pickle.dump(list(set(list(checkpoints) + list(already_computed_checkpoints))), file)\n",
    "\n",
    "        return DictTensorDataset(dataset_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration roneneldan--TinyStories-a62fc98e062666ca\n",
      "Reusing dataset parquet (/home/paperspace/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-a62fc98e062666ca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d18083dc4c6408abbc7b4c87e02c643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset           = load_dataset(\"roneneldan/TinyStories\")\n",
    "test_dataset      = dataset[\"validation\"]\n",
    "train_val_dataset = dataset[\"train\"].train_test_split()\n",
    "train_dataset     = train_val_dataset[\"train\"]\n",
    "val_dataset       = train_val_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dataset = make_tokens_dataset(train_dataset, model.tokenizer, ncontext=250, save_to=\"/storage/tokens_dataset.pickle\") # \"/home/paperspace/tokens_dataset.pickle\")\n",
    "# tokens_dataset = Subset(tokens_dataset, range(100_000)) # not enough ram :( and waiting for access to a machine with a big disk\n",
    "tokens_dataloader = DataLoader(tokens_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_tokens_dataset = make_tokens_dataset(val_dataset, model.tokenizer, ncontext=250, save_to=\"/storage/val_tokens_dataset.pickle\")# \"/home/paperspace/val_tokens_dataset.pickle\")\n",
    "val_tokens_dataloader = DataLoader(val_tokens_dataset, batch_size=64)\n",
    "\n",
    "activations_dataset = make_activation_dataset(model, tokens_dataloader, layers=[0], checkpoints=[\"mlp\"], on_disk=False)# on_disk=True, storage_root_directory=\"/home/paperspace/activations/\")\n",
    "activations_dataloader = DataLoader(activations_dataset, batch_size=4096, collate_fn=dict_collate_fn)\n",
    "\n",
    "val_activations_dataset = make_activation_dataset(model, val_tokens_dataloader, layers=[0], checkpoints=[\"mlp\"], on_disk=False)\n",
    "activations_dataloader = DataLoader(activations_dataset, batch_size=4096, collate_fn=dict_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(Module):\n",
    "    def __init__(self, d, dhidden, activation_function=ReLU()):\n",
    "        super().__init__()\n",
    "        self.pre_bias = Parameter(zeros(d))\n",
    "        self.up = Linear(d, dhidden)\n",
    "        self.activation_function = activation_function\n",
    "        self.down = Linear(dhidden, d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.up(x - self.pre_bias)\n",
    "        hidden = self.activation_function(hidden)\n",
    "        output = self.down(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "class L1Penalty(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.norm(dim=-1, p=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sparse_autoencoder( sparse_autoencoder,\n",
    "                              activations_dataloader,\n",
    "                              checkpoint,\n",
    "                              epochs,\n",
    "                              sparsity_penalty_weight,\n",
    "                              lr=1e-4,\n",
    "                              reconstruction_loss_fn=MSELoss(),\n",
    "                              sparsity_penalty_fn=L1Penalty(),\n",
    "                              optimizer=None,\n",
    "                              epoch_tqdm=True,\n",
    "                              batch_tqdm=False,\n",
    "                              plot=True ):\n",
    "    optimizer = AdamW(sparse_autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    reconstruction_loss_history = []\n",
    "    sparsity_penalty_history = []\n",
    "    for epoch in tqdm(range(epochs)) if epoch_tqdm else range(epochs):\n",
    "        epoch_reconstruction_loss = 0\n",
    "        epoch_sparsity_penalty = 0\n",
    "        for activations in tqdm(activations_dataloader) if batch_tqdm else activations_dataloader:\n",
    "            activations = activations[checkpoint].detach()\n",
    "            reconstructed, hidden = sparse_autoencoder(activations)\n",
    "            reconstruction_loss = reconstruction_loss_fn(reconstructed, activations)\n",
    "            sparsity_penalty = sparsity_penalty_fn(hidden)\n",
    "            loss = reconstruction_loss + sparsity_penalty_weight * sparsity_penalty\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_reconstruction_loss += reconstruction_loss.item()\n",
    "            epoch_sparsity_penalty += sparsity_penalty.item()\n",
    "\n",
    "        reconstruction_loss_history.append(epoch_reconstruction_loss / len(activations_dataloader))\n",
    "        sparsity_penalty_history.append(epoch_sparsity_penalty / len(activations_dataloader))\n",
    "\n",
    "    if plot:\n",
    "        plt.title(\"Sparse autoencoder training loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.plot(reconstruction_loss_history, label=\"reconstruction loss\")\n",
    "        plt.plot(sparsity_penalty_history, label=\"sparsity penalty\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def test_sparse_autoencoder(model, dataloader, autoencoder, checkpoint, reconstruction_loss_fn=KLDivLoss(), sparsity_penalty_fn=L1Penalty(), _tqdm=True):\n",
    "    autoencoders = {checkpoint: autoencoder}\n",
    "\n",
    "    with inference_mode():\n",
    "        reconstruction_loss = 0\n",
    "        sparsity_penalty = 0\n",
    "        for tokens, in tqdm(dataloader) if _tqdm else dataloader:\n",
    "            tokens = tokens.to(device)\n",
    "            result = model(tokens)\n",
    "            result_with_autoencoders = model(tokens, autoencoders=autoencoders, return_autoencoder_activations=True)\n",
    "            reconstruction_loss += reconstruction_loss_fn(result.logits, result_with_autoencoders.logits)\n",
    "            autoencoder_activaitons = result_with_autoencoders.autoencoder_activations[checkpoint]\n",
    "            sparsity_penalty += sparsity_penalty_fn(autoencoder_activaitons)\n",
    "        \n",
    "    return { \"reconstruction_loss\": reconstruction_loss / len(dataloader),\n",
    "             \"sparsity_penalty\":    sparsity_penalty    / len(dataloader) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder = SparseAutoencoder(model.cfg.dmodel, 4*model.cfg.dmodel).to(device)\n",
    "train_sparse_autoencoder(sparse_autoencoder, activations_dataloader, checkpoint=(0, \"mlp\"), epochs=250, lr=1e-3, sparsity_penalty_weight=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sparse_autoencoder(model, val_tokens_dataloader, sparse_autoencoder, checkpoint=(0, \"mlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model - don't look at it, we will only train the model from scratch if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_logits(model, seq):\n",
    "    return model(seq)[..., -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_dataset(vocab_size, ncontext, size):\n",
    "    assert ncontext % 2 == 1\n",
    "    data = randint(vocab_size, (size, (ncontext + 1) // 2), device=device)\n",
    "    data = data.repeat(1, 2)\n",
    "    return TensorDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cross_entropy_loss(pred, true):\n",
    "    return cross_entropy(pred.transpose(1, -1), true.transpose(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, loss_fn=transformer_cross_entropy_loss, lr=1e-3, epoch_tqdm=True, batch_tqdm=False, plot_loss=True):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in tqdm(range(epochs)) if epoch_tqdm else range(epochs):\n",
    "        for x, in tqdm(dataloader) if batch_tqdm else dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x[..., :-1]), x[..., 1:])\n",
    "            loss_history.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.title(\"training_loss\")\n",
    "        plt.xlabel(\"training iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(vocab_size=10, ncontext=17, dmodel=16, dhead=4, nhead=4, dmlp=32, nlayers=2)\n",
    "train_dataloader = DataLoader(repetition_dataset(vocab_size=cfg.vocab_size, ncontext=cfg.ncontext, size=500_000), batch_size=64, shuffle=True)\n",
    "model = Transformer(cfg).to(device)\n",
    "train(model, train_dataloader, epochs=1, batch_tqdm=True, epoch_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(repetition_dataset(vocab_size=cfg.vocab_size, ncontext=cfg.ncontext, size=1_000), batch_size=64, shuffle=True)\n",
    "x, = next(iter(test_dataloader))\n",
    "model(x[0, :2])\n",
    "print(x.shape)\n",
    "print(model(x[..., :-1]).argmax(-1)[0, ...])\n",
    "print(x[..., 1:][0, ...])\n",
    "print(x[..., :-1][0, ...])\n",
    "print(transformer_cross_entropy_loss(model(x[..., :-1]), x[..., 1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
