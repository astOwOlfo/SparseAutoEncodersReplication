{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.7.0)\n",
      "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.9/dist-packages (4.35.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (1.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (4.64.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (0.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (0.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.35.2) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.35.2) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.35.2) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.35.2) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.35.2) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, tensor, arange, randn, randint, tril, where, full_like, ones, allclose, empty, zeros\n",
    "from torch.nn import Module, Linear, GELU, ReLU, Parameter, Embedding, ModuleList, LayerNorm, MSELoss, KLDivLoss\n",
    "from torch.nn.functional import softmax, cross_entropy\n",
    "from torch.nn.init import zeros_\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "# from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Callable\n",
    "from einops import einsum\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isfile\n",
    "from math import sqrt, pi\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int\n",
    "    ncontext: int\n",
    "    dmodel: int\n",
    "    dhead: int\n",
    "    nhead: int\n",
    "    dmlp : int\n",
    "    nlayers: int\n",
    "    activation_function: Callable = GELU()\n",
    "    mask_value: float = 1e-5\n",
    "    attention_scale: float = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.attention_scale is None:\n",
    "            self.attention_scale = 1 / sqrt(self.dhead)\n",
    "\n",
    "def normalize(x, dim=-1, eps=1e-5):\n",
    "    return x / (x.pow(2).mean(dim=dim, keepdim=True) + eps).sqrt()\n",
    "\n",
    "# copied from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "class NewGELUActivation(Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return 0.5 * input * (1.0 + torch.tanh(sqrt(2.0 / pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "@dataclass\n",
    "class BlockActivations:\n",
    "    mid:       Optional[Tensor] = None\n",
    "    post:      Optional[Tensor] = None\n",
    "    attention: Optional[Tensor] = None\n",
    "    mlp:       Optional[Tensor] = None\n",
    "\n",
    "class IdentityAutoencoder(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x, None\n",
    "\n",
    "class BlockAutoencoders:\n",
    "    mid:       Callable = IdentityAutoencoder()\n",
    "    post:      Callable = IdentityAutoencoder()\n",
    "    attention: Callable = IdentityAutoencoder()\n",
    "    mlp:       Callable = IdentityAutoencoder()\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.up = Linear(cfg.dmodel, cfg.dmlp)\n",
    "        self.down = Linear(cfg.dmlp, cfg.dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.cfg.activation_function(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.query_weight  = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.key_weight    = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.value_weight  = Parameter(randn(cfg.nhead, cfg.dmodel, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.output_weight = Parameter(randn(cfg.nhead, cfg.dhead, cfg.dmodel) / sqrt(cfg.nhead * cfg.dhead))\n",
    "\n",
    "        self.query_bias    = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.key_bias      = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.value_bias    = Parameter(randn(cfg.nhead, cfg.dhead) / sqrt(cfg.dmodel))\n",
    "        self.output_bias   = Parameter(randn(cfg.dmodel)           / sqrt(cfg.nhead * cfg.dhead))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ncontext = x.size(-2)\n",
    "\n",
    "        query = einsum(x, self.query_weight, \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        key   = einsum(x, self.key_weight,   \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        value = einsum(x, self.value_weight, \"... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\")\n",
    "        query = query + self.query_bias\n",
    "        key   = key   + self.key_bias\n",
    "        value = value + self.value_bias\n",
    "\n",
    "        attention = einsum(\n",
    "            key,\n",
    "            query,\n",
    "            \"... ncontext_key nhead dhead, ... ncontext_query nhead dhead -> ... nhead ncontext_query ncontext_key\"\n",
    "        )\n",
    "        attention = self.cfg.attention_scale * attention\n",
    "        attention_mask = tril(ones((ncontext, ncontext), dtype=torch.bool, device=device))\n",
    "        attention = where(attention_mask, attention, tensor(self.cfg.mask_value, device=device))\n",
    "        attention = softmax(attention, dim=-1)\n",
    "        \n",
    "        output = einsum(\n",
    "            attention,\n",
    "            value,\n",
    "            \"... nhead ncontext_query ncontext_key, ... ncontext_key nhead dhead -> ... ncontext_query nhead dhead\"\n",
    "        )\n",
    "        result = einsum(output, self.output_weight, \"... ncontext nhead dhead, nhead dhead dmodel -> ... ncontext dmodel\")\n",
    "        result = result + self.output_bias\n",
    "        return result\n",
    "    \n",
    "@dataclass\n",
    "class BlockOutput:\n",
    "    output: Tensor\n",
    "    activations: Optional[BlockActivations] = None\n",
    "    autoencoder_activations: Optional[BlockActivations] = None\n",
    "\n",
    "class TransformerBlock(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.attention_layer_norm = LayerNorm(cfg.dmodel)\n",
    "        self.mlp_layer_norm = LayerNorm(cfg.dmodel)\n",
    "\n",
    "        self.attention = Attention(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, pre, return_activations=False, return_autoencoder_activations=False, autoencoders=BlockAutoencoders()):\n",
    "        attention = self.attention(self.attention_layer_norm(pre))\n",
    "        attention, autoencoder_attention = autoencoders.attention(attention)\n",
    "        mid = pre + attention\n",
    "        mid, autoencoder_mid = autoencoders.mid(mid)\n",
    "        mlp = self.mlp(self.mlp_layer_norm(mid))\n",
    "        mlp, autoencoder_mlp = autoencoders.mlp(mlp)\n",
    "        post = mid + mlp\n",
    "        post, autoencoder_post = autoencoders.post(post)\n",
    "\n",
    "        result = BlockOutput(post)\n",
    "        if return_activations:\n",
    "            result.activations = BlockActivations(mid=mid, post=post, attention=attention, mlp=mlp)\n",
    "        if return_autoencoder_activations:\n",
    "            result.autoencoder_activations = BlockActivations(mid=autoencoder_mid, post=autoencoder_post, attention=autoencoder_attention, mlp=autoencoder_mlp)\n",
    "        return result\n",
    "    \n",
    "@dataclass\n",
    "class TransformerOutput:\n",
    "    logits: Tensor\n",
    "    activations: Optional[List[BlockActivations]] = None\n",
    "    autoencoder_activations: Optional[List[BlockActivations]] = None\n",
    "\n",
    "class Transformer(Module):\n",
    "    def __init__(self, cfg, tokenizer=None):\n",
    "        super().__init__()\n",
    "        if type(cfg.activation_function) == str:\n",
    "            cfg.activation_function = {\"gelu\": GELU(), \"relu\": ReLU(), \"gelu_new\": NewGELUActivation()}[cfg.activation_function]\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.embedding = Embedding(cfg.vocab_size, cfg.dmodel)\n",
    "        self.positional_embedding = Embedding(cfg.ncontext, cfg.dmodel)\n",
    "        self.blocks = ModuleList([TransformerBlock(cfg) for _ in range(cfg.nlayers)])\n",
    "        self.unembedding = Linear(cfg.dmodel, cfg.vocab_size)\n",
    "        self.final_layer_norm = LayerNorm(cfg.dmodel)\n",
    "\n",
    "    def forward(self, x, return_activations=False, return_autoencoder_activations=False, stop_at_layer=None, autoencoders: Dict[int, BlockAutoencoders] = dict()):\n",
    "        if isinstance(x, str):\n",
    "            x = self.tokenizer(x)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        ncontext = x.size(-2)\n",
    "        x = x + self.positional_embedding(arange(ncontext, device=device))\n",
    "        \n",
    "        activations = [] if return_activations else None\n",
    "        autoencoder_activations = [] if return_autoencoder_activations else None\n",
    "        for layer, block in enumerate(self.blocks):\n",
    "            block_autoencoders = autoencoders[layer] if layer in autoencoders else BlockAutoencoders()\n",
    "            result = block(x, return_activations=True, return_autoencoder_activations=return_autoencoder_activations, autoencoders=block_autoencoders)\n",
    "            x = result.output\n",
    "            \n",
    "            if return_activations:\n",
    "                activations.append(result.activations)\n",
    "            if return_autoencoder_activations:\n",
    "                autoencoder_activations.append(result.autoencoder_activations)\n",
    "\n",
    "            if stop_at_layer is not None and layer > stop_at_layer:\n",
    "                break\n",
    "\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.unembedding(x)\n",
    "        \n",
    "        result = TransformerOutput(x)\n",
    "        if return_activations:\n",
    "            result.activations = activations\n",
    "        if return_autoencoder_activations:\n",
    "            result.autoencoder_activations = autoencoder_activations\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name, test=True, test_atol=1e-4):\n",
    "        theirs = AutoModelForCausalLM.from_pretrained(pretrained_model_name).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "        ours = Transformer(TransformerConfig( vocab_size=          tokenizer.vocab_size,\n",
    "                                              ncontext=            theirs.config.max_position_embeddings,\n",
    "                                              dmodel=              theirs.config.hidden_size,\n",
    "                                              dhead=               theirs.config.hidden_size // theirs.config.num_heads,\n",
    "                                              nhead=               theirs.config.num_heads,\n",
    "                                              dmlp=                4 * theirs.config.hidden_size,\n",
    "                                              nlayers=             theirs.config.num_layers,\n",
    "                                              activation_function= theirs.config.activation_function,\n",
    "                                              attention_scale=     1.0,\n",
    "                                              mask_value=          torch.finfo(torch.float).min )).to(device)\n",
    "        \n",
    "        ours.tokenizer = tokenizer\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ours.embedding.weight.copy_(theirs.transformer.wte.weight)\n",
    "            ours.positional_embedding.weight.copy_(theirs.transformer.wpe.weight)\n",
    "            ours.unembedding.weight.copy_(theirs.transformer.wte.weight)\n",
    "            zeros_(ours.unembedding.bias)\n",
    "            ours.final_layer_norm.weight.copy_(theirs.transformer.ln_f.weight)\n",
    "            ours.final_layer_norm.bias.copy_(theirs.transformer.ln_f.bias)\n",
    "                    \n",
    "            for layer in range(ours.cfg.nlayers):\n",
    "                ours.blocks[layer].attention_layer_norm.weight.copy_(theirs.transformer.h[layer].ln_1.weight)\n",
    "                ours.blocks[layer].attention_layer_norm.bias.copy_(theirs.transformer.h[layer].ln_1.bias)\n",
    "                ours.blocks[layer].mlp_layer_norm.weight.copy_(theirs.transformer.h[layer].ln_2.weight)\n",
    "                ours.blocks[layer].mlp_layer_norm.bias.copy_(theirs.transformer.h[layer].ln_2.bias)\n",
    "\n",
    "                ours.blocks[layer].attention.query_weight.copy_(theirs.transformer.h[layer].attn.attention.q_proj.weight.reshape(ours.cfg.nhead, ours.cfg.dhead, ours.cfg.dmodel).permute(0, 2, 1))\n",
    "                ours.blocks[layer].attention.key_weight.copy_(theirs.transformer.h[layer].attn.attention.k_proj.weight.reshape(ours.cfg.nhead, ours.cfg.dhead, ours.cfg.dmodel).permute(0, 2, 1))\n",
    "                ours.blocks[layer].attention.value_weight.copy_(theirs.transformer.h[layer].attn.attention.v_proj.weight.reshape(ours.cfg.nhead, ours.cfg.dhead, ours.cfg.dmodel).permute(0, 2, 1))\n",
    "                ours.blocks[layer].attention.output_weight.copy_(theirs.transformer.h[layer].attn.attention.out_proj.weight.reshape(ours.cfg.dmodel, ours.cfg.nhead, ours.cfg.dhead).permute(1, 2, 0))\n",
    "\n",
    "                zeros_(ours.blocks[layer].attention.query_bias)\n",
    "                zeros_(ours.blocks[layer].attention.key_bias)\n",
    "                zeros_(ours.blocks[layer].attention.value_bias)\n",
    "                ours.blocks[layer].attention.output_bias.copy_(theirs.transformer.h[layer].attn.attention.out_proj.bias)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.weight.copy_(theirs.transformer.h[layer].mlp.c_fc.weight)\n",
    "                ours.blocks[layer].mlp.down.weight.copy_(theirs.transformer.h[layer].mlp.c_proj.weight)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.bias.copy_(theirs.transformer.h[layer].mlp.c_fc.bias)\n",
    "                ours.blocks[layer].mlp.down.bias.copy_(theirs.transformer.h[layer].mlp.c_proj.bias)\n",
    "            \n",
    "\n",
    "        if test:\n",
    "            print(\"Testing that the model behaves the same as the library model... \", end=\"\", flush=True)\n",
    "            inputs = randint(0, ours.cfg.vocab_size, (64, 64), device=device)\n",
    "            assert allclose(ours(inputs).logits, theirs(inputs).logits, atol=test_atol), \"Tests failed!\"\n",
    "            print(\"Test passed!\")\n",
    "\n",
    "        return ours\n",
    "    \n",
    "        \"\"\"\n",
    "        theirs = HookedTransformer.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        ours = Transformer(TransformerConfig( vocab_size=theirs.tokenizer.vocab_size,\n",
    "                                      ncontext=theirs.cfg.n_ctx,\n",
    "                                      dmodel=theirs.cfg.d_model,\n",
    "                                      dhead=theirs.cfg.d_head,\n",
    "                                      nhead=theirs.cfg.n_heads,\n",
    "                                      dmlp=theirs.cfg.d_mlp,\n",
    "                                      nlayers=theirs.cfg.n_layers,\n",
    "                                      activation_function=theirs.cfg.act_fn ))\n",
    "\n",
    "        ours.tokenizer = theirs.tokenizer\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ours.embedding.weight.copy_(theirs.embed.W_E)\n",
    "            ours.positional_embedding.weight.copy_(theirs.pos_embed.W_pos)\n",
    "            ours.unembedding.weight.copy_(theirs.unembed.W_U.transpose(0, 1))\n",
    "            ours.unembedding.bias.copy_(theirs.unembed.b_U)\n",
    "            \n",
    "            for layer in range(ours.cfg.nlayers):\n",
    "                ours.blocks[layer].attention.query_weight.copy_(theirs.blocks[layer].attn.W_Q)\n",
    "                ours.blocks[layer].attention.key_weight.copy_(theirs.blocks[layer].attn.W_K)\n",
    "                ours.blocks[layer].attention.value_weight.copy_(theirs.blocks[layer].attn.W_V)\n",
    "                ours.blocks[layer].attention.output_weight.copy_(theirs.blocks[layer].attn.W_O)\n",
    "\n",
    "                ours.blocks[layer].attention.query_bias.copy_(theirs.blocks[layer].attn.b_Q)\n",
    "                ours.blocks[layer].attention.key_bias.copy_(theirs.blocks[layer].attn.b_K)\n",
    "                ours.blocks[layer].attention.value_bias.copy_(theirs.blocks[layer].attn.b_V)\n",
    "                ours.blocks[layer].attention.output_bias.copy_(theirs.blocks[layer].attn.b_O)\n",
    "\n",
    "                ours.blocks[layer].mlp.up.weight.copy_(theirs.blocks[layer].mlp.W_in.transpose(0, 1))\n",
    "                ours.blocks[layer].mlp.down.weight.copy_(theirs.blocks[layer].mlp.W_out.transpose(0, 1))\n",
    "\n",
    "                ours.blocks[layer].mlp.up.bias.copy_(theirs.blocks[layer].mlp.b_in)\n",
    "                ours.blocks[layer].mlp.down.bias.copy_(theirs.blocks[layer].mlp.b_out)\n",
    "\n",
    "        return ours\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing that the model behaves the same as the library model... Test passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(50257, 256)\n",
       "  (positional_embedding): Embedding(2048, 256)\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention()\n",
       "      (mlp): MLP(\n",
       "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (unembedding): Linear(in_features=256, out_features=50257, bias=True)\n",
       "  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer.from_pretrained(\"roneneldan/TinyStories-8M\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset, val_size=0.1, test_size=0.1):\n",
    "    # assert set(dataset.keys()) == {\"train\"}\n",
    "    dataset = dataset.train_test_split(test_size=val_size+test_size)\n",
    "    val_test_dataset = dataset[\"test\"].train_test_split(test_size = val_size / (val_size + test_size))\n",
    "    return DatasetDict({ \"train\": dataset[\"train\"],\n",
    "                         \"val\":   val_test_dataset[\"train\"],\n",
    "                         \"test\":  val_test_dataset[\"test\"] })\n",
    "\n",
    "def make_tokens_dataset(text_dataset, tokenizer, ncontext, _tqdm=True, max_size=None, save_to=None):\n",
    "    if save_to is not None and isfile(save_to):\n",
    "        print(f\"Loading tokens dataset from file '{save_to}'.\")\n",
    "        return torch.load(save_to)\n",
    "    \n",
    "    if _tqdm and max_size is not None:\n",
    "        print(\"WARNING: tqdm doesn't work properly when max_size is not None\")\n",
    "\n",
    "    token_seqs = []\n",
    "    for x in tqdm(text_dataset) if _tqdm else text_dataset:\n",
    "        tokens = tokenizer(x[\"text\"])[\"input_ids\"]\n",
    "        if len(tokens) <= ncontext:\n",
    "            continue\n",
    "        tokens = tokens[:ncontext]\n",
    "        token_seqs.append(tokens)\n",
    "        \n",
    "        if max_size is not None and len(token_seqs) >= max_size:\n",
    "            break\n",
    "    \n",
    "    dataset = TensorDataset(tensor(token_seqs, device=device, requires_grad=False))\n",
    "\n",
    "    if save_to is not None:\n",
    "        torch.save(dataset, save_to)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class BlockActivationsDataset(Dataset):\n",
    "    def __init__(self, activations: BlockActivations):\n",
    "        self.activations = activations\n",
    "\n",
    "    def __len__(self):\n",
    "        for attr in [\"mid\", \"post\", \"attention\", \"mlp\"]:\n",
    "            activation = getattr(self.activations, attr)\n",
    "            if activation is not None:\n",
    "                return activation.size(0)\n",
    "        assert False\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        slice = BlockActivations(None, None, None, None)\n",
    "        for attr in [\"mid\", \"post\", \"attention\", \"mlp\"]:\n",
    "            activation = getattr(self.activations, attr)\n",
    "            if activation is not None:\n",
    "                setattr(slice, attr, activation[index])\n",
    "        return slice\n",
    "\n",
    "class ActivationsDataset(Dataset):\n",
    "    def __init__(self, activations: Dict[int, BlockActivations]):\n",
    "        self.activations = { layer: BlockActivationsDataset(layer_activations)\n",
    "                             for layer, layer_activations in activations.items() }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.activations.values())))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {layer: activations[index] for layer, activations in self.activations.items()}\n",
    "\n",
    "def block_activations_collate_fn(activations: List[BlockActivations]):\n",
    "    checkpoints = [checkpoint for checkpoint in [\"mid\", \"post\", \"attention\", \"mlp\"] if getattr(activations[0], checkpoint) is not None]\n",
    "    shape = (len(activations), *getattr(activations[0], checkpoints[0]).shape)\n",
    "    result = BlockActivations(**{checkpoint: empty(shape, device=device, requires_grad=False) for checkpoint in checkpoints})\n",
    "    for i, acts in enumerate(activations):\n",
    "        for checkpoint in checkpoints:\n",
    "            getattr(result, checkpoint)[i] = getattr(acts, checkpoint)\n",
    "    return result\n",
    "\n",
    "def activations_collate_fn(activations: List[Dict[int, BlockActivations]]):\n",
    "    return {layer: block_activations_collate_fn([acts[layer] for acts in activations]) for layer in activations[0].keys()}\n",
    "\n",
    "def make_activation_dataset(model, dataloader, layers, checkpoints, _tqdm=True, save_to=None):\n",
    "    if save_to is not None and isfile(save_to):\n",
    "        print(f\"Loading activations dataset from file '{save_to}'.\")\n",
    "        return torch.load(save_to)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_activations = None\n",
    "        i = 0\n",
    "        for data, in tqdm(dataloader) if _tqdm else dataloader:\n",
    "            output = model(data, return_activations=True, stop_at_layer=max(layers))\n",
    "            \n",
    "            if all_activations is None:\n",
    "                shape = (len(dataloader.dataset), *output.activations[0].mid.shape[1:])\n",
    "                all_activations = { layer: BlockActivations(**{checkpoint: empty(shape, requires_grad=False) for checkpoint in checkpoints})\n",
    "                                    for layer in layers }\n",
    "                \n",
    "            batch_size = output.activations[0].mid.size(0)\n",
    "            for layer in layers:\n",
    "                for checkpoint in checkpoints:\n",
    "                    getattr(all_activations[layer], checkpoint)[i:i+batch_size] = getattr(output.activations[layer], checkpoint)\n",
    "            \n",
    "            i += batch_size\n",
    "\n",
    "    dataset = ActivationsDataset(all_activations)\n",
    "    assert i == len(dataset)\n",
    "\n",
    "    if save_to is not None:\n",
    "        torch.save(dataset, save_to)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration roneneldan--TinyStories-a62fc98e062666ca\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-a62fc98e062666ca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2dac30b5d741d4a437d90fef384537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "val_test_dataset = dataset[\"validation\"].train_test_split(test_size=0.5)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset   = val_test_dataset[\"train\"]\n",
    "test_dataset  = val_test_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokens dataset from file '/storage/tokens_dataset.pickle'.\n"
     ]
    }
   ],
   "source": [
    "tokens_dataset = make_tokens_dataset(train_dataset, model.tokenizer, ncontext=250, save_to=\"/storage/tokens_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokens dataset from file '/storage/tokens_dataset.pickle'.\n",
      "Loading tokens dataset from file '/storage/test_tokens_dataset.pickle'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [03:28<00:00,  7.50it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_dataset = make_tokens_dataset(train_dataset, model.tokenizer, ncontext=250, save_to=\"/storage/tokens_dataset.pickle\")\n",
    "tokens_dataset = Subset(tokens_dataset, range(100_000))\n",
    "tokens_dataloader = DataLoader(tokens_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_tokens_dataset = make_tokens_dataset(val_dataset, model.tokenizer, ncontext=250, save_to=\"/storage/test_tokens_dataset.pickle\")\n",
    "val_tokens_dataloader = DataLoader(val_tokens_dataset, batch_size=64)\n",
    "\n",
    "activations_dataset = make_activation_dataset(model, tokens_dataloader, layers=[0], checkpoints=[\"mlp\"]) # , save_to=\"/storage/activations_dataset.pickle\")\n",
    "activations_dataloader = DataLoader(activations_dataset, batch_size=64, collate_fn=activations_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the transformer from here on - ignore this, we will only do this at the end if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_logits(model, seq):\n",
    "    return model(seq)[..., -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_dataset(vocab_size, ncontext, size):\n",
    "    assert ncontext % 2 == 1\n",
    "    data = randint(vocab_size, (size, (ncontext + 1) // 2), device=device)\n",
    "    data = data.repeat(1, 2)\n",
    "    return TensorDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cross_entropy_loss(pred, true):\n",
    "    return cross_entropy(pred.transpose(1, -1), true.transpose(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, loss_fn=transformer_cross_entropy_loss, lr=1e-3, epoch_tqdm=True, batch_tqdm=False, plot_loss=True):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in tqdm(range(epochs)) if epoch_tqdm else range(epochs):\n",
    "        for x, in tqdm(dataloader) if batch_tqdm else dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x[..., :-1]), x[..., 1:])\n",
    "            loss_history.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.title(\"training_loss\")\n",
    "        plt.xlabel(\"training iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/7813 [00:00<02:24, 54.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 552/7813 [00:05<01:18, 92.57it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(repetition_dataset(vocab_size\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mvocab_size, ncontext\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mncontext, size\u001b[39m=\u001b[39m\u001b[39m500_000\u001b[39m), batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Transformer(cfg)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(model, train_dataloader, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_tqdm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epoch_tqdm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, \u001b[39min\u001b[39;00m tqdm(dataloader) \u001b[39mif\u001b[39;00m batch_tqdm \u001b[39melse\u001b[39;00m dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(model(x[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_history\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_activations:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m layer \u001b[39m>\u001b[39m stop_at_layer:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pre, return_activations\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(normalize(pre))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautoencoders\u001b[39m.\u001b[39mattention(attention)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     mid \u001b[39m=\u001b[39m pre \u001b[39m+\u001b[39m attention\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     ncontext \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     query \u001b[39m=\u001b[39m einsum(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_weight, \u001b[39m\"\u001b[39;49m\u001b[39m... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     key   \u001b[39m=\u001b[39m einsum(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_weight,   \u001b[39m\"\u001b[39m\u001b[39m... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vladimir/ml/sparse-autoencoder/sparse_autoencoder.ipynb#X16sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     value \u001b[39m=\u001b[39m einsum(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_weight, \u001b[39m\"\u001b[39m\u001b[39m... ncontext dmodel, nhead dmodel dhead -> ... ncontext nhead dhead\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py:901\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    899\u001b[0m tensors \u001b[39m=\u001b[39m tensors_and_pattern[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    900\u001b[0m pattern \u001b[39m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n\u001b[0;32m--> 901\u001b[0m \u001b[39mreturn\u001b[39;00m get_backend(tensors[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49meinsum(pattern, \u001b[39m*\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/einops/_backends.py:287\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, pattern, *x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, pattern, \u001b[39m*\u001b[39mx):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(pattern, \u001b[39m*\u001b[39;49mx)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = TransformerConfig(vocab_size=10, ncontext=17, dmodel=16, dhead=4, nhead=4, dmlp=32, nlayers=2)\n",
    "train_dataloader = DataLoader(repetition_dataset(vocab_size=cfg.vocab_size, ncontext=cfg.ncontext, size=500_000), batch_size=64, shuffle=True)\n",
    "model = Transformer(cfg).to(device)\n",
    "train(model, train_dataloader, epochs=1, batch_tqdm=True, epoch_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 18])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 2, 5, 2, 1, 3, 1, 6])\n",
      "tensor([8, 2, 5, 2, 1, 3, 1, 6, 1, 8, 2, 5, 2, 1, 3, 1, 6])\n",
      "tensor([1, 8, 2, 5, 2, 1, 3, 1, 6, 1, 8, 2, 5, 2, 1, 3, 1])\n",
      "tensor(1.0842, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(repetition_dataset(vocab_size=cfg.vocab_size, ncontext=cfg.ncontext, size=1_000), batch_size=64, shuffle=True)\n",
    "x, = next(iter(test_dataloader))\n",
    "model(x[0, :2])\n",
    "print(x.shape)\n",
    "print(model(x[..., :-1]).argmax(-1)[0, ...])\n",
    "print(x[..., 1:][0, ...])\n",
    "print(x[..., :-1][0, ...])\n",
    "print(transformer_cross_entropy_loss(model(x[..., :-1]), x[..., 1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
